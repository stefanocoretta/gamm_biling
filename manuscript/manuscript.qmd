---
title: "A tutorial on Generalised Additive Mixed Effects Models for bilingualism research"
author: Stefano Coretta, Joseph V. Casillas
format: pdf
editor: visual
bibliography: gam_refs.bib
csl: apa.csl
---

```{r}
#| label: setup
#| include: false

library(tidyverse)
theme_set(theme_light())
library(mgcv)
library(tidygam)
library(dagitty)

dat1 <- readRDS("data/dat1.rds")
dat2 <- readRDS("data/dat2.rds")

```

## Motivation

Quantitative data analysis of linguistic data has seen a massive shift from statistical significance tests and analysis of variance to linear and generalised linear modelling, including models with so-called random effects (also known as hierarchical, mixed-effects, multi-level or nested models) [@winter2013; @winter2020]. Soon after linear models started being more commonly used, a need for models that could handle non-linear effects and were as flexible as linear models led to the adoption of Generalised Linear (Mixed) Models, or GAMMs.

GAMMs are now commonly employed in phonetic research (given the naturally dynamic nature of speech data) and this is reflected by the availability of several tutorials which focus on phonetic data [@soskuthy2017; @soskuthy2021; @wieling2018; @tamminga2016]. This is why we hope that this tutorial will make GAMMs accessible to researchers from other fields within linguistics. In particular, this tutorial is thought for researchers in bilingualism and multilingualism who wish to be able to start using GAMMs for non-linear data, which is very common in developmental and learning phenomena.

### Generalised additive (mixed) models

Generalised Additive Models (GAMs) and their mixed-effects version (GAMMs) are a generalisation of generalised linear models that can accommodate fitting non-linear models [@hastie1986; @wood2006]. The main component that is specific to GAMMs are smoothers or smooth terms. These are model terms that can fit non-linear effects. An advantage of using smooth terms is that they are constrained by a penalty measure---part of the fitting procedure and does not depend on user input---which prevents over-fitting the data. This approach is different from, for example, growth curve analyses which are based on user-specified polynomial degrees.

Readers interested in a more detailed conceptual discussion of GAMMs are referred to @soskuthy2017. The rest of this tutorial will focus on teaching the basics of GAMMs usage in R. While completing this tutorial will allow you to start-jump into fitting basic GAMMs, we invite interested readers to peruse the tutorials mentioned above to avoid unnecessary repetition of content. Finally, we opted not to treat statistical significance testing in GAMMs, due to its cumbersome nature (significance testing is treated in details in the aforementioned tutorials).

## Pre-requisites

This tutorial assumes readers are already familiar with R [@rct2023] and have at least some experience fitting linear models, including models with random effects. The following packages need to be installed:

-   tidyverse

-   mgcv

-   tidygam

You can find the tutorial code and data here: XXX.

## Case study 1: U-shaped learning

### U-shaped learning

Research on language acquisition first documented over-regularization errors in the 60's [@ervin_miller1963; @cazden1968acquisition]. Subsequent research found that over-regularization errors tended to manifest in a developmental curve, and is now referred to as the *U-shaped developmental curve* [@williams2022u]. In most cases, U-shaped development is considered to be a three-step process, which begins with accurate performance, followed by a period in which performance dips, and then subsequently becomes accurate again [@carlucci2013necessity]. Interestingly, U-shaped curves are observed in numerous cognitive-developmental and learning contexts, particularly with regard to child development, e.g., understanding temperature, weight conservation, object permanence, and facial recognition [See @carlucci2013necessity], which, in turn, has fostered fruitful research and debate in Cognitive Science regarding learning models [e.g., @marcus1992overregularization; @rumelhart2014learning, among others].

In first language acquisition research, the most cited example stems from children's use of the past tense [@ervin_miller1963; @cazden1968acquisition; @marcus1992overregularization]. For instance, @cazden1968acquisition found that children accurately produced irregular past tense verbs during a period of time, and, at a later point in time, began to produce over-generalization errors, e.g., *feet* followed by \**feets*. Eventually past tense production again surfaces as expected, typically around the age of three, which coincides with the developmental stage in which children acquire regular past tense verbs [See @marcus1992overregularization]. In the realm of Second Language Acquisition (SLA), researchers have documented non-linear, U-shaped courses of development in numerous areas for both developmental and instructional sequences [See @kellerman1985if; @long1990least; @shirai1990u; @geeslin2006longitudinal; @pliatsikas2013processing; @casani2020valutare; @williams2022u, among many others]. @kellerman1985if, for instance, documented Dutch learners of English acquiring verb alternation patterns of English following a U-shaped trajectory. In a similar vein, @lightbown1983exploring showed accuracy producing progressive forms (i.e., *-ing*) followed a U-shaped trajectory when acquiring the simple present/present continuous tense/aspect distinction in French speaking learners of English. U-shaped learning may be a by-product of reorganization or restructuring of prior knowledge [@gass_selinker_2008; @shirai1990u], and likely accounts for language instructors' contention that students often regress when learning new linguistic forms. Be that as it may, it is clear that the non-linear quality of this aspect of language learning requires a non-linear approach to modelling the underlying process.

### The data

We have simulated data of learning scores from 200 subjects, taken at 10 time points. A proficiency score was also included for each subject at each time point. The data is intended to simulate a study in which participants perform a learning task and take a proficiency test at 10 consecutive time points. This is what the data looks like.

```{r}
#| label: dat1

dat1
```

The `score` column contains the learning scores, while the `proficiency` column the proficiency scores. The subject ID is given in `subj`. time point 0 to 9 is in `time`. @fig-dat1 shows the relationship between proficiency and learning scores for individual subjects. From the figure, it is clear that such relationship has a U-shape, by which learning scores intially decrease, plateau, then increase again with higher proficiency.

```{r}
#| label: fig-dat1
#| echo: false

dat1 %>%
  ggplot(aes(proficiency, score, group = subj)) +
  geom_path(alpha = 0.2) +
  geom_point(alpha = 0.2) +
  labs(
    x = "Proficiency score",
    y = "Learning score",
    caption = "Connected points are measurements that belong\nto a single subject, taken at each of the 10 time points."
  )

```

In the following sections we will analyse this data using GAMMs. For pedagogical purposes, we will first focus on the effect of proficiency scores on learning scores at a single time point, time point 5. Subsequently, we will analyse the entire data, to illustrate how to conduct a time-series analysis.

### Modelling a non-linear effect

Let's first focus on how to model a non-linear effect: here, we can look at the effect of the proficiency score on the learning score. As we have seen in @fig-dat1, the effect is U-shaped, i.e. it is not linear. To simplify things, let's look only at data from time point. We will extend the analysis to also include time point as a predictor later. We can model a non-linear effect of proficiency on the learning score with the following code.

```{r}
#| label: gam-1

dat15 <- filter(dat1, time == 5)

# attach the mgcv package
library(mgcv)

# fit the model
gam_1 <- gam(
  score ~ s(proficiency),
  data = dat15
)

```

The formula states that `score`, the outcome variable (also known as the dependent variable) should be modelled as a function of `proficiency`, but we use the `s()` to indicate that we want to estimate a (potentially) non-linear effect. The name of the function, `s`, stands for "smooth term". Smooth terms (aka smoothers) are mathematical objects that allow GAMs to fit non-linear effects. A detailed treatment of smoothers is beyond the scope of this tutorial, so we refer the readers to XXX.

Before looking at the summary of the `gam_1` model, let's plot the predicted effect of proficiency. We will use the tidygam package, which provides users with utility functions that make extracting and plotting predictions from GAMs easier.

```{r}
#| label: fig-gam-1

# attach tidygam
library(tidygam)

# extract model predictions
gam_1_preds <- predict_gam(gam_1)

# plot predictions
plot(gam_1_preds, series = "proficiency")

```

@fig-gam-1 is a plot of the predicted effect of proficiency on learning score, based on the `gam_1` model from above. We will look into the details of `predict_gam()`. For now, just notice that the learning score initially decreases with increasing proficiency until about 0.5 and then starts increasing, thus producing the typical U-shaped curve. Let's inspect the model summary now.

```{r}
#| label: gam-1-sum

summary(gam_1)
```

The relevant parts of the summary for the time being are the `Parametric coefficients:` table and the `Approximate significance of smooth terms:` table. The first table contains the estimate of the intercept. This intercept is the same intercept you would get in a linear model: here, the estimate of the intercept is the predicted learning score when proficiency is 0. According to the summary, when proficiency is 0, the learning score is about 25. But what we are really interested in is the effect of proficiency on learning score, rather than the intercept per se.

Information on the effect of proficiency is found in the second table, which contains estimates of the smooth terms. Alas, these estimates don't say much about the effect per se. Rather, they just indicate if the effect is linear or not. More specifically, the `edf` estimate, which stands for Estimated Degrees of Freedom, would be 1 for perfectly linear effects and greater than 1 for non-linear effects. This number tells us nothing about the shape of the effect. The only way to assess this is to plot the model predictions, as we have done above.

In our `gam_1` model, the `edf` for the smooth term over proficiency is 2.9. This is significantly different from 1 (p \< 2e-16), thus suggesting that the effect of proficiency is not linear (the `Ref.df`, reference degrees of freedom, and `F` values have the only function of being needed to get a *p*-value).

If we were to report this model and results, we could write something along the following lines: We fitted a Generalised Additive Model to learning score, with a smooth term over proficiency (to model non-linear effects). According to the model, the effect of proficiency is significantly non-linear (*F* = 16.46, *p* \< 0.0001). Based on the prediction plot, we observe that at lower proficiency, learning scores initially decrease and then start increasing again from about proficiency 0.5.

In the following section we will refit the data now including time point (note that this would be the model to fit in the first place and we have fitted a model only with proficiency for pedagogical purposes).

### Multiple smooth terms

The data `dat1` contains learning and proficiency scores from 200 subjects, taken at 10 time points. The data was simulated so that proficiency increased with time (at different degrees for different speakers). An interesting question is whether learning scores improve with time independently of proficiency, or if proficiency alone is causing learning scores to improve. We can approach with question by applying a statistical method called causal inference, based on directed acyclic graphs (DAGs) theory (a full treatment of this is beyond the scope of the paper. Readers are referred to XXX).

The DAGs in @fig-d1-dag represent the causal relationships between learning scores, proficiency scores and time point in the two scenarios. In (a), time point affects proficiency and proficiency affects learning scores. In other words, time point has a direct effect on proficiency but not on learning scores; learning scores can be predicted from proficiency alone. In (b), on the other hand, time point affects proficiency as in (a) but also learning scores (and proficiency affects learning scores as in (a)). This means that time point affects learning scores in two ways: through its effect on proficiency and directly through its effect on learning scores.

```{r}
#| label: fig-d1-dag
#| fig-cap: "Dyrected Acyclic Graphs for time, proficiency and learning score."
#| fig-subcap:
#| - "No direct effect of time on learning scores."
#| - "Direct effect of time on learning scores."
#| layout-ncol: 2
#| fig-width: 3
#| fig-height: 1.5
#| echo: false

g1 <- dagitty('dag {
proficiency [exposure,pos="-0.677,-0.105"]
score [outcome,pos="-0.070,0.385"]
time [pos="0.529,-0.098"]
proficiency -> score
time -> proficiency
}
')

exposures(g1) <- "proficiency"
outcomes(g1) <- "score"
plot(g1)

g2 <- dagitty('dag {
proficiency [exposure,pos="-0.677,-0.105"]
score [outcome,pos="-0.070,0.385"]
time [pos="0.529,-0.098"]
proficiency -> score
time -> proficiency
time -> score
}
')

plot(g2)
```

DAGs allow us to make causal statements based on statistical results. In this case, when including both time point and proficiency as predictors in a GAM, time should not have an effect on learning score if scenario (a) is correct (while it should have an effect if scenario (b) is correct). Let's model the data.

```{r}
#| label: gam-2
#| cache: true

gam_2 <- gam(
  score ~ s(proficiency) + s(time) +
    s(subj, bs = "re"),
  data = dat1
)
```

In `gam_2` we fit a GAM to learning scores `score` with two predictors: a smooth term over proficiency and a smooth term over time point. We also include random effects to account for data from multiple participants. The syntax for random effects in GAMs is different from the syntax in lme4. With `gam()`, we can specify random effects using smooth terms and the `re` (for Random Effects) basis function. Random intercept are added with the syntax `s(ranint, bs = "re")` and random slopes with the syntax `s(ranint, ranslope, bs = "re")`. Here we just add a random intercept by subject for illustration. Let's inspect the model summary.

```{r}
#| label: gam-2-sum

summary(gam_2)
```

...

Let's plot the model predictions.

```{r}
#| label: gam-2-preds

gam_2_preds <- predict_gam(
  gam_2, length_out = 25,
  series = "proficiency",
  exclude_terms = c("s(subj)", "s(subj,proficiency)")
)

plot(gam_2_preds)
```

```{r}
#| label: gam-2-preds-t

gam_2_preds_t <- predict_gam(
  gam_2, length_out = 25,
  series = "time",
  exclude_terms = c("s(subj)", "s(subj,proficiency)")
)

plot(gam_2_preds_t) +
  ylim(20, 45)
```

## Case study 2: simultaneous vs late bilinguals

To motivate our next example, we will now consider the production of Spanish vowels in three groups of bilinguals: simultaneous/native English/Spanish bilinguals (henceforth `es`) and late learners of Spanish with low and high levels of proficiency, `es_0` and `es_1`, respectively. Spanish has five phonemic vowels, /i, e, a, o, u/, and is often described as being a syllable-timed language in which the spectral envelope is rather stable and stressed and unstressed syllables have approximately the same duration. Though there is evidence that unstressed vowels can centralize to a certain degree, the differences between stressed and unstressed vowels are believed to be imperceptible [@martinez_celdran_1984]. American English, on the other hand, has a larger vowel inventory and is often described as a stress-timed language in which the duration between stressed syllables is approximately the same. Vowel reduction can occur in unstressed syllables, which typically manifests via shortening and/or centralization, with unstressed vowels often reducing to schwa ([ə]). Accordingly, English speaking learners of Spanish face a substantial obstacle when it comes to producing and perceiving Spanish vowels [See @aldrich2014acquisition; @cobb2009pronunciacion; @cobb2015adult; @bland2016speech; @iruela1997adquisicion, among others]. To wit, they often display cross-linguistic influence by producing unstressed Spanish vowels with \[ə\], particularly in the case of /a/, e.g., "casa" (Eng. *house*) as \[ka.sə\], and diphthongizing vowels in final position [@cobb2015adult].

In the following example, we consider the production of Spanish vowels in the aforementioned groups of bilingual speakers. Interestingly, to our knowledge, all of the current research investigating the cross-linguistic influence of vowel reduction processes in learners of Spanish have utilized modelling strategies that rely upon formant measurements at a single time point, typically the mid-point of the vowel, rather than scrutinizing the entire trajectory of the spectral envelope. In our view, the dynamic nature of vowel formants poses an interesting use-case for modelling using non-linear methods, such as GAMs. To this end, we have simulated phonetic data from three groups of bilinguals: simultaneous English/Spanish bilinguals, beginners late English/Spanish bilinguals and advanced late English/Spanish bilinguals. The data contains the Euclidean Distance (EuD) of three Spanish vowels /a, i, u/ from the vowel space centroid, taken from word-final non-stressed vowels, along 9 equidistant time points. The measure is a good proxy for vowel reduction phenomena, which is to be expected in late bilinguals.

```{r}
#| label: dat2

dat2
```

This is what the data looks like. The top row reports the synthetic data of simultaneous English/Spanish bilinguals (`es`). It can be observed that the Euclidean distance is quite stable across the duration of the vowel for all three vowels and away from 0 (which corresponds to a mid-central vowel), indicating no reduction takes place (as it is to be expected for Spanish). On the other hand, in the second and third row, we can see some vowel reduction at play: for /a/, beginners late bilinguals (`es_0`) completely reduce the vowel to a mid-central vowel, while the advanced late bilinguals (`es_1`) reduce it to a lesser degree, but it is still not quite a full Spanish /a/; /i/ and /u/ are produced with a diphthongal quality, where the first part of the vowel is quite reduced. Finally, comparing beginners and advanced bilinguals shows that the latter have less reduction, as expected by increasing proficiency in Spanish.

```{r}
#| label: fig-dat2
#| echo: false

dat2 %>%
  ggplot(aes(timep, eud, group = id, colour = group)) +
  geom_path(alpha = 0.15, linewidth = 1) +
  facet_grid(cols = vars(vowel), rows = vars(group))
```

We can use a GAM to model Euclidean distance along time point in the different vowels and different bilingual groups. Fitting smooth terms for different levels of categorical predictors can be achieved by specifying the categorical predictor as the value of the `by` argument inside the smooth term function `s()`: for example `s(timep, by = vowel)` would ensure a non-linear effect of time point is estimated for each vowel. In our data, we want to model the interaction of vowel (/a, i, u/) and group (`es, es_0, es_1`). Unfortunately, classical interactions cannot be model in GAMs (because of the "additive" aspect of the model; interactions require product operations).

To obviate this limitation, we can simply construct a new predictor with the combination of vowel and group and use that as the `by`-variable in the smooth terms. Due to how smooth terms are constructed when you include a `by`-variable, it is necessary to also include the variable as a parametric term (i.e. a classical linear term).

```{r}
#| label: dat2-prep

dat2 <- dat2 %>%
  mutate(
    vowel = as.factor(vowel),
    group_vowel = interaction(group, vowel),
  )
```

```{r}
#| label: gam-3
#| eval: false

gam_3 <- gam(
  eud ~
    # Parametric term for group_vowel
    group_vowel +
    # Smooth term with group_vowel by-variable
    s(timep, by = group_vowel, k = 9) +
    # Random factor smooths
    s(timep, subj, bs = "fs", m = 1),
  data = dat2
)
```

```{r}
#| label: gam-3-eval
#| echo: false
gam_3_path <- "data/gam_3.rds"
if (file.exists(gam_3_path)) {
  gam_3 <- readRDS(gam_3_path)
} else {
  gam_3 <- gam(
    eud ~
      # Parametric term for group_vowel
      group_vowel +
      # Smooth term with group_vowel by-variable
      s(timep, by = group_vowel, k = 9) +
      # Random factor smooths
      s(timep, subj, bs = "fs", m = 1),
    data = dat2
  )
  saveRDS(gam_3, gam_3_path)
}
```

```{r}
#| label: gam-3-sum

summary(gam_3)
```

Let's extract the model predictions and inspect them.

```{r}
#| label: gam-3-preds

gam_3_preds <- predict_gam(gam_3, exclude_terms = "s(timep,subj)")
gam_3_preds
```

The function `predict_gam()` automatically samples 10 equidistant points from numeric variables (like `timep`, based on the default `length_out = 10` argument) and all levels in categorical variables (like `group_vowel`). For each combination of the sampled points and levels, the function returns the value of the outcome (`eud`) and the standard error (`se`). The lower (`lower_ci`) and upper (`upper_ci`) boundaries of the 95% Confidence Interval are also returned.

Furthermore, `predict_gam()` returns an object of class `tidygam` which can be plotted with `plot()` (similarly to how `ggpredict()` from the ggeffects package works, XXX). However, we might want to do some processing of the predictions before plotting: in particular, we might want to split the `group_vowel` column into the two original variables, `group` and `vowel`. We can achieve this straight from the function. And we might also want to extract more than 10 time points, to get a smoother curve.

```{r}
#| label: gam-3-preds-2

gam_3_preds_2 <- predict_gam(
  gam_3,
  length_out = 25,
  exclude_terms = "s(timep,subj)",
  separate = list(group_vowel = c("group", "vowel"))
)
gam_3_preds_2
```

The syntax `list(group_vowel = c("group", "vowel"))` instructs the function to split the `group_vowel` column into two columns, `group` and `vowel`. Splitting is done based on the `sep_by` argument, which is `.` by default. Now we can plot the predictions.

```{r}
#| label: fig-gam-3
#| fig-cap: "Predicted Euclidean distance along the duration of the vowel, for /a, i, u/ in simultaneous bilinguals (es), beginners late bilinguals (es_0) and advanced late bilinguals (es_1)"

gam_3_preds_2 %>%
  plot(series = "timep", comparison = "group") +
  facet_grid(cols = vars(vowel))
```

{{< pagebreak >}}

# References
